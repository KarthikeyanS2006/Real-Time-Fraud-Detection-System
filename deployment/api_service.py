# deployment/api_service.py
"""
Conceptual FastAPI service to serve the real-time fraud model.
This mock demonstrates how the trained model is loaded once and used
to score incoming transactions via a low-latency API endpoint.
"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
import pandas as pd
import time
import os

# --- Configuration (Matching fraud_system.py) ---
MODEL_FILENAME = 'models/fraud_detection_xgb_model.joblib'
SCALER_FILENAME = 'models/scaler.joblib'
NUMERICAL_FEATURES = ['amount', 'hour_of_day', 'distance_from_last_tx', 'count_tx_1h']
PREDICTION_THRESHOLD = 0.80

# --- Pydantic Schema for Input Data ---
# Defines the expected structure of an incoming transaction request
class TransactionRequest(BaseModel):
    user_id: str
    amount: float
    distance_from_last_tx: float
    hour_of_day: int
    count_tx_1h: int # This would normally come from the Feature Store lookup

# --- API Initialization ---
app = FastAPI(title="Real-Time Fraud Scorer")

# Global variables to hold the loaded assets
model_scorer = None
scaler_preprocessor = None

# --- Mock Feature Store (For demonstration in the API context) ---
MOCK_FEATURE_STORE = {} # In production, this would be a Redis client

# --- Startup Event: Load Model Assets ---
@app.on_event("startup")
def load_model_assets():
    """Load the trained model and scaler when the FastAPI service starts."""
    global model_scorer, scaler_preprocessor
    
    # Check if files exist (assuming they were generated by fraud_system.py)
    if not os.path.exists(MODEL_FILENAME):
        raise RuntimeError("Model file not found. Run training first.")
    
    print("Loading model and scaler for API service...")
    model_scorer = joblib.load(MODEL_FILENAME)
    scaler_preprocessor = joblib.load(SCALER_FILENAME)
    print("Model loaded successfully. Ready for inference.")

# --- Prediction Endpoint ---
@app.post("/api/v1/predict_fraud")
def predict(tx_data: TransactionRequest):
    start_time = time.time()
    
    # 1. Prepare Data for Model (including Feature Store Mock for this demo)
    # NOTE: In a real system, the 'count_tx_1h' would be looked up from Redis
    
    input_data = tx_data.dict()
    X_predict = pd.DataFrame([
        {k: input_data[k] for k in NUMERICAL_FEATURES}
    ])
    
    # 2. Scale Features
    X_scaled = scaler_preprocessor.transform(X_predict)
    X_final = pd.DataFrame(X_scaled, columns=NUMERICAL_FEATURES)
    
    # 3. Model Inference
    try:
        score = model_scorer.predict_proba(X_final)[:, 1][0]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Inference error: {e}")

    # 4. Decision Engine Logic
    decision = 'ACCEPT'
    if score >= PREDICTION_THRESHOLD:
        decision = 'BLOCK'
    elif score >= (PREDICTION_THRESHOLD * 0.5):
        decision = 'FLAG for Review'
        
    latency_ms = (time.time() - start_time) * 1000
    
    return {
        "transaction_id": "mock_tx_" + str(int(time.time() * 1000)),
        "fraud_score": round(score, 4),
        "decision": decision,
        "inference_latency_ms": round(latency_ms, 2)
    }

# To run this mock API: uvicorn api_service:app --reload